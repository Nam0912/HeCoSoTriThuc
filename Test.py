import requests
from googletrans import Translator
from collections import defaultdict

translator = Translator()

# C√°c quan h·ªá h·ªØu √≠ch
useful_relations = {"UsedFor", "MadeOf", "PartOf", "IsA"}

# T·ª´ r√°c n√™n lo·∫°i b·ªè
ban_words = {"thing", "object", "something", "someone", "money", "news", "page",
             "marker", "note", "card", "booklet", "item"}


def tra_cuu_conceptnet(concept_en, limit=100):
    """
    Truy v·∫•n ConceptNet ƒë√∫ng ƒë·ªãnh d·∫°ng /c/en/{t·ª´}
    v√† tr·∫£ v·ªÅ danh s√°ch (start, rel, end)
    """
    url = f"https://api.conceptnet.io/c/en/{concept_en}?offset=0&limit={limit}"
    try:
        data = requests.get(url).json()
    except Exception as e:
        print("‚ö†Ô∏è L·ªói truy c·∫≠p API:", e)
        return []

    edges = []
    for edge in data.get("edges", []):
        rel = edge["rel"]["label"]
        if rel not in useful_relations:
            continue

        start = edge["start"]["label"].lower()
        end = edge["end"]["label"].lower()

        # Ch·ªâ l·∫•y kh√°i ni·ªám ti·∫øng Anh
        if not (edge["start"]["@id"].startswith("/c/en/") and edge["end"]["@id"].startswith("/c/en/")):
            continue

        # Lo·∫°i b·ªè t·ª´ r√°c
        if any(bad in start for bad in ban_words) or any(bad in end for bad in ban_words):
            continue

        edges.append((start, rel, end))
    return edges


def sinh_luat_tu_conceptnet(ds_tu_viet, limit_per_word=100):
    """Sinh lu·∫≠t tri th·ª©c v·∫≠t‚Äìh√†nh ƒë·ªông ho·∫∑c v·∫≠t‚Äìch·∫•t li·ªáu."""
    en_list = [translator.translate(t.strip(), src="vi", dest="en").text.lower() for t in ds_tu_viet]
    print(f"üîç ƒêang sinh tri th·ª©c cho: {en_list}")

    rules = []

    for concept in en_list:
        edges = tra_cuu_conceptnet(concept, limit_per_word)
        for s, rel, e in edges:
            # n·∫øu ƒë·ªì v·∫≠t ƒë∆∞·ª£c l√†m t·ª´ ch·∫•t li·ªáu
            if rel == "MadeOf":
                rules.append(f"{e} -> {s}")
            # n·∫øu d√πng ƒë·ªÉ l√†m g√¨
            elif rel == "UsedFor":
                rules.append(f"{e} -> {s}")
            # n·∫øu l√† m·ªôt ph·∫ßn c·ªßa
            elif rel == "PartOf":
                rules.append(f"{s} -> {e}")
            # n·∫øu l√† ph√¢n c·∫•p
            elif rel == "IsA":
                rules.append(f"{s} -> {e}")

    # lo·∫°i tr√πng
    rules = sorted(set(rules))

    # l∆∞u file
    with open("knowledge_base.txt", "w", encoding="utf-8") as f:
        for r in rules:
            f.write(r + "\n")

    print(f"\nüíæ ƒê√£ sinh {len(rules)} lu·∫≠t v√†o 'knowledge_base.txt'")

    # hi·ªÉn th·ªã b·∫£n d·ªãch m·∫´u
    if rules:
        print("\nüìò M·ªôt s·ªë lu·∫≠t m·∫´u (d·ªãch sang ti·∫øng Vi·ªát):")
        for rule in rules[:8]:
            left, right = rule.split(" -> ")
            vi_left = translator.translate(left, src="en", dest="vi").text
            vi_right = translator.translate(right, src="en", dest="vi").text
            print(f"- {vi_left} -> {vi_right}")

    return rules


# =========================
# CH·∫†Y TH·ª¨
# =========================
if __name__ == "__main__":
    tu_nhap = input("Nh·∫≠p c√°c kh√°i ni·ªám (c√°ch nhau b·ªüi d·∫•u ph·∫©y): ")
    ds = [x.strip() for x in tu_nhap.split(",") if x.strip()]
    sinh_luat_tu_conceptnet(ds)
